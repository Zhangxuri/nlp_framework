data: data/trg_data/dd/
logF: experiments/dd/
model: seq2seq
epoch: 20
batch_size: 64
optim: 'adam'
cell: 'lstm'
attention: 'luong_gate'
learning_rate: 0.0003
max_grad_norm: 10
learning_rate_decay: 0.5
start_decay_at: 6
emb_size: 512
hidden_size: 512
dec_num_layers: 3
enc_num_layers: 3
bidirectional: True
dropout: 0.0
max_time_step: 50
eval_interval: 674
save_interval: 674
metrics: ['bleu']
shared_vocab: True
beam_size: 10
unk: True
schedule: False
selfatt: True
schesamp: False
swish: True
length_norm: True
