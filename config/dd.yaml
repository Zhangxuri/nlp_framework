data: data/trg_data/dd/
logF: experiments/dd/
model: seq2seq
epoch: 40
batch_size: 64
optim: 'adam'
cell: 'gru'
attention: 'luong'
learning_rate: 0.0003
max_grad_norm: 10
learning_rate_decay: 0.5
start_decay_at: 10
emb_size: 512
hidden_size: 512
dec_num_layers: 1
enc_num_layers: 1
bidirectional: True
dropout: 0.0
max_time_step: 50
eval_interval: 674
save_interval: 674
metrics: ['bleu']
beam_size: 1
unk: True
schedule: False
selfatt: False
schesamp: False
swish: False
length_norm: True
shared_vocab: True